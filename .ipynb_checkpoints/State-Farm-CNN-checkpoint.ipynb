{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs/state-farm\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ubuntu/nbs/state-farm/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\n",
      "├── \u001b[01;34mresults\u001b[00m\n",
      "│   └── \u001b[01;34mmodel_feat.dat\u001b[00m\n",
      "│       ├── \u001b[01;34mdata\u001b[00m\n",
      "│       └── \u001b[01;34mmeta\u001b[00m\n",
      "├── \u001b[01;34msample\u001b[00m\n",
      "│   ├── \u001b[01;34mtest\u001b[00m\n",
      "│   ├── \u001b[01;34mtrain\u001b[00m\n",
      "│   │   ├── \u001b[01;34mc0\u001b[00m\n",
      "│   │   ├── \u001b[01;34mc1\u001b[00m\n",
      "│   │   ├── \u001b[01;34mc2\u001b[00m\n",
      "│   │   ├── \u001b[01;34mc3\u001b[00m\n",
      "│   │   ├── \u001b[01;34mc4\u001b[00m\n",
      "│   │   ├── \u001b[01;34mc5\u001b[00m\n",
      "│   │   ├── \u001b[01;34mc6\u001b[00m\n",
      "│   │   ├── \u001b[01;34mc7\u001b[00m\n",
      "│   │   ├── \u001b[01;34mc8\u001b[00m\n",
      "│   │   └── \u001b[01;34mc9\u001b[00m\n",
      "│   └── \u001b[01;34mvalid\u001b[00m\n",
      "│       ├── \u001b[01;34mc0\u001b[00m\n",
      "│       ├── \u001b[01;34mc1\u001b[00m\n",
      "│       ├── \u001b[01;34mc2\u001b[00m\n",
      "│       ├── \u001b[01;34mc3\u001b[00m\n",
      "│       ├── \u001b[01;34mc4\u001b[00m\n",
      "│       ├── \u001b[01;34mc5\u001b[00m\n",
      "│       ├── \u001b[01;34mc6\u001b[00m\n",
      "│       ├── \u001b[01;34mc7\u001b[00m\n",
      "│       ├── \u001b[01;34mc8\u001b[00m\n",
      "│       └── \u001b[01;34mc9\u001b[00m\n",
      "├── \u001b[01;34mtest\u001b[00m\n",
      "│   └── \u001b[01;34munknown\u001b[00m\n",
      "├── \u001b[01;34mtrain\u001b[00m\n",
      "│   ├── \u001b[01;34mc0\u001b[00m\n",
      "│   ├── \u001b[01;34mc1\u001b[00m\n",
      "│   ├── \u001b[01;34mc2\u001b[00m\n",
      "│   ├── \u001b[01;34mc3\u001b[00m\n",
      "│   ├── \u001b[01;34mc4\u001b[00m\n",
      "│   ├── \u001b[01;34mc5\u001b[00m\n",
      "│   ├── \u001b[01;34mc6\u001b[00m\n",
      "│   ├── \u001b[01;34mc7\u001b[00m\n",
      "│   ├── \u001b[01;34mc8\u001b[00m\n",
      "│   └── \u001b[01;34mc9\u001b[00m\n",
      "└── \u001b[01;34mvalid\u001b[00m\n",
      "    ├── \u001b[01;34mc0\u001b[00m\n",
      "    ├── \u001b[01;34mc1\u001b[00m\n",
      "    ├── \u001b[01;34mc2\u001b[00m\n",
      "    ├── \u001b[01;34mc3\u001b[00m\n",
      "    ├── \u001b[01;34mc4\u001b[00m\n",
      "    ├── \u001b[01;34mc5\u001b[00m\n",
      "    ├── \u001b[01;34mc6\u001b[00m\n",
      "    ├── \u001b[01;34mc7\u001b[00m\n",
      "    ├── \u001b[01;34mc8\u001b[00m\n",
      "    └── \u001b[01;34mc9\u001b[00m\n",
      "\n",
      "52 directories\n"
     ]
    }
   ],
   "source": [
    "!tree -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional\n",
    "from keras.layers import TimeDistributed, Activation, SimpleRNN, GRU\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.regularizers import l2, activity_l2, l1, activity_l1\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.utils.layer_utils import layer_from_config\n",
    "from keras.metrics import categorical_crossentropy, categorical_accuracy\n",
    "from keras.layers.convolutional import *\n",
    "from keras.preprocessing import image, sequence\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path= \"/home/ubuntu/nbs/state-farm/\"\n",
    "# path = \"/home/ubuntu/nbs/state-farm/sample/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Making Data Augmentation Generator\n",
    "daGen = ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing A Generator\n",
    "gen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initizializing Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18161 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Setting Training Batches\n",
    "batches = daGen.flow_from_directory(path+'train',target_size=(224,224),\n",
    "                                    class_mode='categorical', shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4263 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "#Setting Valid Batches\n",
    "val_batches = gen.flow_from_directory(path+'valid',target_size=(224,224), \n",
    "                                        class_mode='categorical', shuffle=False, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "#Setting Valid Batches\n",
    "test_batches = gen.flow_from_directory(path+'test',target_size=(224,224), \n",
    "                                        class_mode='categorical', shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Model and Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Model Layers\n",
    "model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "            Convolution2D(16,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D(),\n",
    "            Convolution2D(32,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D(),\n",
    "            Convolution2D(32,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D(),\n",
    "            Flatten(),\n",
    "            Dense(200, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.5),\n",
    "            Dense(500, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Competition is evaluating on multi-categorical log loss so compile with categorical_crossentropy\n",
    "model.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save Weights So You Can Return to Them Later \n",
    "model.model.save_weights(path+'/results/State-Farm-CNN-Weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load Weights From Previous Session\n",
    "model.model.load_weights(path+'/results/State-Farm-CNN-Weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just some of the epochs/learning rates I ran on this particular model. In reality, I probably trained somewhere between 30-45 epochs at various learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "18161/18161 [==============================] - 377s - loss: 0.4008 - acc: 0.8689 - val_loss: 1.2051 - val_acc: 0.6725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f43a0392690>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit (train) Your Model!\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=1, validation_data=val_batches,\n",
    "                    nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "18161/18161 [==============================] - 292s - loss: 0.3379 - acc: 0.8902 - val_loss: 1.1595 - val_acc: 0.6721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f438c075e90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(Adam(lr=0.000001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=1, validation_data=val_batches,\n",
    "                    nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "18161/18161 [==============================] - 292s - loss: 0.3677 - acc: 0.8784 - val_loss: 1.2462 - val_acc: 0.6596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f43824d0fd0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=1, validation_data=val_batches,\n",
    "                    nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your model is trained, you can go ahead and generate predictions on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.44287160e-05,   3.54882592e-04,   8.63014211e-05, ...,\n",
       "          1.75091967e-01,   4.34493572e-02,   9.73060280e-02],\n",
       "       [  2.00760961e-01,   1.71303819e-03,   4.67287350e-07, ...,\n",
       "          2.09232712e-05,   8.27580379e-07,   2.40275962e-03],\n",
       "       [  8.22647451e-07,   2.66220354e-07,   2.10584785e-05, ...,\n",
       "          7.34573646e-09,   1.88846093e-06,   5.55211255e-09],\n",
       "       ..., \n",
       "       [  9.60235047e-06,   8.23140363e-05,   5.99129010e-07, ...,\n",
       "          8.27723416e-05,   1.01545397e-02,   4.86909645e-03],\n",
       "       [  8.05833399e-01,   4.40675532e-03,   8.47985211e-04, ...,\n",
       "          9.02374304e-05,   2.13639032e-05,   2.30352692e-02],\n",
       "       [  2.77498420e-02,   3.03828201e-05,   6.49756767e-05, ...,\n",
       "          1.05576322e-03,   9.01309322e-05,   2.29713251e-03]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bcolz\n",
    "def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname): return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/State-Farm-CNN-Results.dat', predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have the data you need to submit to this Kaggle competition. You're a few data wrangling commands away from loading your predictions into a CSV in the proper format and submitting!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
